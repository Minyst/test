{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8296cb2-b267-45d4-8ff7-c1bb5d7285c6",
   "metadata": {},
   "source": [
    "## 테스트 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ede7c6f-a53b-40f2-a1c1-903b65209208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27a1113e-711d-4ed2-8647-0cb7a6452681",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/workspace/test/test.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/workspace/test/test.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      2\u001b[0m     test_dataset \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:308\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/workspace/test/test.json'"
     ]
    }
   ],
   "source": [
    "file_path = '/workspace/test/test.json'\n",
    "with open(file_path, 'r', encoding=\"utf-8\" ) as file:\n",
    "    test_dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1878c4-960e-46d6-a8ae-aeaae24da760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model tokenizer load\n",
    "model = 'meta-llama/Llama-3.1-8B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdf5301-112c-4cbb-86f6-3625b7530104",
   "metadata": {},
   "source": [
    "테스트 데이터를 불러와서 입력과 레이블 형태로 전처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c862f-35ea-47a4-9986-f3f6c4d04b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = []\n",
    "label_lst = []\n",
    "\n",
    "for prompt in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        prompt, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    input = text.split('<|im_start|>assistant')[0] + '<|im_start|>assistant'\n",
    "    label = text.split('<|im_start|>assistant')[1]\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e61704-92d2-4dc9-badc-3bbdf1392a0f",
   "metadata": {},
   "source": [
    "임의로 20번 샘플을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d2cd7-a64e-425c-8a06-5700c820fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20번 테스트 샘플 출력\n",
    "print(prompt_lst[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68475cf6-6696-4def-aee3-8558af6a0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 20번 테스트 샘플 레이블 출력\n",
    "print(label_lst[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae03139-f90e-4e95-bb08-af23071d9b09",
   "metadata": {},
   "source": [
    "## 기본 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d0276b-17c0-454c-86db-65cb6aa1378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline 객체와 LLM의 입력을 넣으면 LLM의 답변을 출력해주는 함수\n",
    "def test_inference(pipe, prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabc4c4-04d6-4e99-90b7-07d669d71f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B', device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "eos_token = tokenizer(\"<|im_end|>\", add_special_tokens=False)[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b9a774-645c-4e5b-9661-0cba55778124",
   "metadata": {},
   "source": [
    "기본 모델에 임의로 20번 샘플을 넣어서 출력합니다. 기본 모델은 출처를 남기지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90e035a-5396-4bb7-8516-0112eabdb070",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_inference(pipe, prompt_lst[20])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642dab44-f3da-4d46-92c5-a13cb6164c8e",
   "metadata": {},
   "source": [
    "기본 모델에 임의로 400~404번 샘플을 출력합니다. 기본 모델은 지시사항에서 출처를 남기라는 지시를 따르지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f812b-aba1-4197-9c5f-87e069fb6a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, label in zip(prompt_lst[400:405], label_lst[400:405]):\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eea975-5f02-47c6-8230-d90a35eeca88",
   "metadata": {},
   "source": [
    "## 튜닝 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5cfd7-6bff-4fce-b59d-b53281cebc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "fine_tuned_pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)\n",
    "eos_token = tokenizer(\"<|im_end|>\", add_special_tokens=False)[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd0c139-3e37-4e0f-82c3-838698611697",
   "metadata": {},
   "source": [
    "튜닝 모델에 임의로 20번 샘플을 넣어서 출력합니다. 튜닝 모델은 출처를 남깁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5f8c5-86df-4eb4-8acc-38370ef822f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_inference(fine_tuned_pipe, prompt_lst[20])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb85cb2d-720b-4413-8da3-03ff17220946",
   "metadata": {},
   "source": [
    "튜닝 모델에 임의로 400~404번 샘플을 출력합니다. 튜닝 모델은 튜닝이 되었으므로 프롬프트 지시사항에 따라서 출처를 남기는 모습을 보여줍니다.  \n",
    "물론, 출처를 남기지 않는 샘플들도 존재할 것입니다. 이는 학습이 덜 된 것이므로 학습 데이터의 비율을 높이고 에포크를 높이시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969c8f49-2fca-4f4e-ba2b-e48c3ac9b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, label in zip(prompt_lst[400:405], label_lst[400:405]):\n",
    "    print(f\"    response:\\n{test_inference(fine_tuned_pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2aaf77-6bac-4f3a-a01d-05b4c29bb4d0",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eea3a7-8df7-40c7-aaa8-496053bee96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python merge.py \\\n",
    "    --base_model_name_or_pathmeta-llama/Llama-3.1-8B-Instruct \\\n",
    "    --peft_model_path ./meta-llama/Llama-3.1-8B-Instruct/checkpoint-177 \\\n",
    "    --output_dir ./output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db3535-0bc7-46b9-86cc-6ef9ccf8ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습 후에는 상단의 Kernel > Shut Down Kernel을 눌러서 종료합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
