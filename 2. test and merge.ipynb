{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8296cb2-b267-45d4-8ff7-c1bb5d7285c6",
   "metadata": {},
   "source": [
    "## 테스트 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ede7c6f-a53b-40f2-a1c1-903b65209208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27a1113e-711d-4ed2-8647-0cb7a6452681",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory test_dataset not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_dataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py:2152\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2150\u001b[0m fs, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m url_to_fs(dataset_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_INFO_FILENAME)) \u001b[38;5;129;01mand\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(\n\u001b[1;32m   2154\u001b[0m     posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_STATE_JSON_FILENAME)\n\u001b[1;32m   2155\u001b[0m ):\n\u001b[1;32m   2156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory test_dataset not found"
     ]
    }
   ],
   "source": [
    "test_dataset = load_from_disk('test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1878c4-960e-46d6-a8ae-aeaae24da760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model tokenizer load\n",
    "model = 'meta-llama/Llama-3.1-8B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdf5301-112c-4cbb-86f6-3625b7530104",
   "metadata": {},
   "source": [
    "테스트 데이터를 불러와서 입력과 레이블 형태로 전처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c862f-35ea-47a4-9986-f3f6c4d04b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = []\n",
    "label_lst = []\n",
    "\n",
    "for prompt in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        prompt, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    input = text.split('<|im_start|>assistant')[0] + '<|im_start|>assistant'\n",
    "    label = text.split('<|im_start|>assistant')[1]\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e61704-92d2-4dc9-badc-3bbdf1392a0f",
   "metadata": {},
   "source": [
    "임의로 20번 샘플을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d2cd7-a64e-425c-8a06-5700c820fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20번 테스트 샘플 출력\n",
    "print(prompt_lst[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68475cf6-6696-4def-aee3-8558af6a0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 20번 테스트 샘플 레이블 출력\n",
    "print(label_lst[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae03139-f90e-4e95-bb08-af23071d9b09",
   "metadata": {},
   "source": [
    "## 기본 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d0276b-17c0-454c-86db-65cb6aa1378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline 객체와 LLM의 입력을 넣으면 LLM의 답변을 출력해주는 함수\n",
    "def test_inference(pipe, prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabc4c4-04d6-4e99-90b7-07d669d71f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2-7B-Instruct', device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "eos_token = tokenizer(\"<|im_end|>\", add_special_tokens=False)[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b9a774-645c-4e5b-9661-0cba55778124",
   "metadata": {},
   "source": [
    "기본 모델에 임의로 20번 샘플을 넣어서 출력합니다. 기본 모델은 출처를 남기지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90e035a-5396-4bb7-8516-0112eabdb070",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_inference(pipe, prompt_lst[20])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642dab44-f3da-4d46-92c5-a13cb6164c8e",
   "metadata": {},
   "source": [
    "기본 모델에 임의로 400~404번 샘플을 출력합니다. 기본 모델은 지시사항에서 출처를 남기라는 지시를 따르지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f812b-aba1-4197-9c5f-87e069fb6a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, label in zip(prompt_lst[400:405], label_lst[400:405]):\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eea975-5f02-47c6-8230-d90a35eeca88",
   "metadata": {},
   "source": [
    "## 튜닝 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5cfd7-6bff-4fce-b59d-b53281cebc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"qwen2-7b-rag-ko/checkpoint-177\"\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "fine_tuned_pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)\n",
    "eos_token = tokenizer(\"<|im_end|>\", add_special_tokens=False)[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd0c139-3e37-4e0f-82c3-838698611697",
   "metadata": {},
   "source": [
    "튜닝 모델에 임의로 20번 샘플을 넣어서 출력합니다. 튜닝 모델은 출처를 남깁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5f8c5-86df-4eb4-8acc-38370ef822f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_inference(fine_tuned_pipe, prompt_lst[20])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb85cb2d-720b-4413-8da3-03ff17220946",
   "metadata": {},
   "source": [
    "튜닝 모델에 임의로 400~404번 샘플을 출력합니다. 튜닝 모델은 튜닝이 되었으므로 프롬프트 지시사항에 따라서 출처를 남기는 모습을 보여줍니다.  \n",
    "물론, 출처를 남기지 않는 샘플들도 존재할 것입니다. 이는 학습이 덜 된 것이므로 학습 데이터의 비율을 높이고 에포크를 높이시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969c8f49-2fca-4f4e-ba2b-e48c3ac9b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, label in zip(prompt_lst[400:405], label_lst[400:405]):\n",
    "    print(f\"    response:\\n{test_inference(fine_tuned_pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2aaf77-6bac-4f3a-a01d-05b4c29bb4d0",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eea3a7-8df7-40c7-aaa8-496053bee96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python merge.py \\\n",
    "    --base_model_name_or_path Qwen/Qwen2-7B-Instruct \\\n",
    "    --peft_model_path ./qwen2-7b-rag-ko/checkpoint-177 \\\n",
    "    --output_dir ./output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db3535-0bc7-46b9-86cc-6ef9ccf8ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습 후에는 상단의 Kernel > Shut Down Kernel을 눌러서 종료합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
